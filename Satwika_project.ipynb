{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-dimensional data is fairly common nowadays. It can be defined as a data set where the number of regressors/predictors (P) is greater than the number of observations (N). High-dimensional data could be a result of two phenomena: <br>\n",
    "1. When the number of characteristics per observation is larger than the sample size. Example: when the medical tests per patient are larger than the chosen sample size <br>  \n",
    "2. When the researcher includes a large number of variable transformations in the regression. Example: since the true functional form is unknown, the researcher adds interaction terms, variables with functional transformation, etc. such that the number of regressors is greater than the number of observations. <br>\n",
    "**What if $P>N$?** <br>\n",
    "When the number of regressors is greater than the number of observations, the regression cannot be identified using OLS <br>\n",
    "**What if $P=N$?** <br>\n",
    "When the number of regressors is equal to the number of observations, the OLS fits the data perfectly such that $R^2=1$ <br>\n",
    "**What if $P<N$?** <br>\n",
    "When the number of regressors is smaller than the number of observations, we will not only be able to identify the model but also avoid overfitting issues as otherwise. When we have high-dimensional data, we need to reduce the dimensions (regularize) such that $P<N$ to draw meaningful conclusions from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Approximately Sparse Regression Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors consider the following model to forecast the outcome ($y_i$) and treat $g(w_i)$ as a high-dimensional, linear model such that <br>\n",
    "<br>\n",
    "$$y_i=g(w_i)+\\zeta_i \\hspace{0.1cm}, \\hspace{0.25cm} i=1,...n \\hspace{0.25cm} \\& \\hspace{0.25cm} E(\\zeta_i|w_i)=0$$ <br>\n",
    "$$g(w_i) =\\sum_{j=1}^{p}{\\beta_jx_{i,j}}+r_{p,i} \\hspace{0.25cm}$$\n",
    "<div style=\"text-align: justify\">   \n",
    "$r_{p,i}$ is an approximation error. To identify the model, avoid overfitting and for better prediction ability, we need to regularize the function $g(\\cdot)$. Typically, researchers regularize based on economic theory and intuition. However, the question of the correct variables and the transformations being chosen is left unanswered. Another approach to reduce dimensions is the approximate sparsity of the high-dimensional linear model. <br>\n",
    "</div>\n",
    "<blockquote>“Approximate sparsity imposes a restriction that only $s$ variables among all of $x_{i,j}$, where $s$ is much smaller than $n$, have associated coefficients $\\beta_j$ that are different from $0$, while permitting a nonzero approximation error $r_{p,i}$.” </blockquote>\n",
    "This implies only S of the P regressors have coefficients that are not equal to zero and $S<P<N$. One of the methods for estimating the parameter of sparse high-dimensional models is the Least Absolute Shrinkage and Selection Operator (LASSO). In this paper, the authors use a variant of LASSO estimator defined as: <br> \n",
    "<br>    \n",
    "$$\\hat{\\beta}=\\mathop{\\rm argmax}\\limits_b\\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}x_{i,j}b_j)^2+\\lambda\\sum_{j=1}^{p}|b_j|\\gamma_j \\hspace{0.25cm}$$\n",
    "<br>  \n",
    "$\\lambda>0$ is the penalty and $\\gamma_j$ are the penalty loadings. LASSO leads to some of the coefficients being set exactly to zero. Thus, it could be used in variable selection process by simply selecting the variables with nonzero coefficients. This addresses the problem when researcher doesn’t know a priori exactly which variables should in the model. <br> \n",
    "<br>      \n",
    "    \n",
    "This variant is proposed in [Belloni, Chen, Chernozhukov, and Hansen (2012)](https://www.econometricsociety.org/publications/econometrica/2012/11/01/sparse-models-and-methods-optimal-instruments-application)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Model Selection when the goal is Causal Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aim of this paper is to use data mining procedures like LASSO for regularizing high-dimensional data to produce meaningful inferences. Authors stress that when the end goal is causal inference, LASSO should be used only for regularization purposes i.e, to only select variables whose coefficients are nonzero but not to interpret their coefficients directly. This is because: <br> \n",
    "1. Such procedures are designed for forecasting purposes and not to inference about model parameters <br> \n",
    "2. Model selection mistakes occur which then might lead to the problem of omitted variable bias <br>  \n",
    "\n",
    "Thus, naively using results obtained from LASSO estimation might lead to inference problems. The authors explain when and how to use LASSO when the end goal of the researcher is causal inference about model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1. Inference with Selection among many Instruments (Z)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors consider the following linear instrumental variable model <br>\n",
    "$$y_i=\\alpha d_i+\\varepsilon_i \\hspace{0.25cm} where \\hspace{0.25cm} d_i={z_i}^{'}\\Pi+r_i+\\nu_i$$ <br>\n",
    "$$E(\\varepsilon_i|z_i)=E(\\nu_i|z_i,r_i)=0 \\hspace{0.25cm} but \\hspace{0.25cm} E(\\varepsilon_i|\\nu_i)\\ne0$$ <br>\n",
    "$d_i$ is an endogenous variable, $z_i$ is a p-dimensional vector of instruments and $r_i$ is an approximation error. In this scenario, the number of instruments is greater than the number of observations $(Z>N)$ which calls for selection among the instruments to be able to estimate $\\alpha$ <br>\n",
    "\n",
    "**Choosing instruments in case of one endogenous variable:** <br>\n",
    "Step 1: Use LASSO to regress $d_i$ (endogenous variable) on $z_i$ (instrumental variables) <br>\n",
    "Step 2: Select the instrument(s) with nonzero coefficients from step 1 <br>\n",
    "Step 3: Carry out standard 2SLS estimation using the selected instrument(s) from step 2 <br>\n",
    "\n",
    "**Choosing instruments in case of more than one endogenous variable:** <br>\n",
    "Step 1: Use LASSO to regress $d_i$ on $z_i$ for each of the endogenous variables <br>\n",
    "Step 2: Select the instrument(s) with nonzero coefficients from step 1 for each of the endogenous variables <br>\n",
    "Step 3: Carry out standard 2SLS estimation using the union of the selected instruments from step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2. Inference with Selection among many Controls (C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors consider the following linear model where a treatment variable, $d_i$, is taken as exogenous after conditioning on control variables <br>\n",
    "\n",
    "\n",
    "$$\\tag{1}\\label{eq:1}y_i=\\alpha d_i+ x_i^{'}\\theta_y+r_{yi}+\\zeta_i \\hspace{0.1cm} ,\\hspace{0.25cm} E(\\zeta_i|d_i,x_i,r_{yi})=0$$\n",
    "<div style=\"text-align: justify\"> $x_i$ is a p-dimensional vector of controls and $r_{yi}$ is an approximation error. In this scenario, the number of controls is greater than the number of observations $(C>N)$ which calls for selection among the controls to be able to estimate $\\alpha$ which is the effect of the treatment on the outcome. To ensure $\\alpha$ remains in the model as it is the parameter of interest, one should exclude the treatment variable from the model selection process. Thus, regularization will only be carried out on the remaining exogenous variables (controls) which are referred to as ‘nuisance’ by the authors. </div> <br>\n",
    "\n",
    "**Naïve Approach for Selection among many Controls (C)**  <br>\n",
    "Step 1: Use LASSO to regress $y_i$ (outcome variable) on $x_i$ (control variables) <br>\n",
    "Step 2: Select the control(s) with nonzero coefficients from step 1 <br>\n",
    "Step 3: Carry out standard OLS estimation using the selected control(s) from step 2\n",
    "<div style=\"text-align: justify\">Running LASSO with $y_i$ on $x_i$ (step 1) only allows us to select those variables with nonzero coefficients associated to $y_i$. However, it ignores the relationship between treatment variable and controls leading to omitted variable bias. This is a major drawback of using the naïve approach. To understand the relationship between treatment variable and controls, the following equations are introduced by the authors </div> <br>\n",
    "\n",
    "$$\\tag{2}\\label{eq:2}d_i=x_i^{'}\\theta_d+r_{di}+\\nu_i \\hspace{0.1cm} ,\\hspace{0.25cm} E(\\nu_i|x_i,r_{di})=0$$ <br>\n",
    "\n",
    "$$\\tag{3}\\label{eq:3}y_i=x_i^{'}(\\alpha\\theta_d+\\theta_y)+(\\alpha r_{di}+r_{yi})+(\\alpha\\nu_i+\\zeta_i)=x_i^{'}\\pi+r_{ci}+\\varepsilon_i \\hspace{0.1cm} ,\\hspace{0.25cm} E(\\varepsilon_i|x_i,r_{ci})=0$$ <br>\n",
    "\n",
    "<div style=\"text-align: justify\">Substituting equation \\eqref{eq:2} in \\eqref{eq:1} will yield \\eqref{eq:3}. $r_{ci}$ is a composite approximation error. Spare high-dimensional methods work best for predictions and not inferences. Equation \\eqref{eq:1} represents a structural model where the goal to is to learn the causal inference. Whereas, equations \\eqref{eq:2} and \\eqref{eq:3} represent predictive relationships, which can be estimated by high-dimensional methods. Thus, to counter against the omitted variable bias, we use both the equations to select the controls. This is called the Post-Double Selection (PDS) LASSO.</div> <br>\n",
    "\n",
    "**Post-Double-Selection (PDS) for Selection among many Controls (C)** <br>\n",
    "Step 1: Use LASSO to regress $y_i$ (outcome variable) on $x_i$ (control variables) <br>\n",
    "Step 2: Select the control(s) with nonzero coefficients from step 1 <br>\n",
    "Step 3: Use LASSO to regress $d_i$ (treatment variable) on $x_i$ (control variables) <br>\n",
    "Step 4: Select the control(s) with nonzero coefficients from step 3 <br>\n",
    "Step 5: Carry out standard OLS estimation using the union of selected controls from step 2 and step 4 <br>\n",
    "\n",
    "<img src= \"simulation.png\" width =\"500\" height=500>\n",
    "\n",
    "Figure 1 is a simulation study from [Belloni, Chernozhukov, and Hansen (2014)](https://academic.oup.com/restud/article-abstract/81/2/608/1523757?redirectedFrom=fulltext). It compares the sampling distribution of the estimator of $\\alpha$ from naïve approach and PDS approach. The left panel suffers from the omitted variable bias which is absent in the right panel.<br>\n",
    "The authors use three empirical examples in this paper and implement the LASSO for selecting controls and instruments in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Empirical Example 1: Estimating the Impact of Eminent Domain on House Prices**\n",
    "\n",
    "*Chen, Daniel L., and Susan Yeh. 2012. [“Growth under the Shadow of Expropriation? The Economic Impacts of Eminent Domain.”](http://www.sole-jole.org/13463.pdf)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
