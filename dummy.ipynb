{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-dimensional data is fairly common nowadays. It can be defined as a data set where the number of regressors/predictors (P) is greater than the number of observations (N). High-dimensional data could be a result of two phenomena: <br>\n",
    "1. When the number of characteristics per observation is larger than the sample size. Example: when the medical tests per patient are larger than the chosen sample size <br>  \n",
    "2. When the researcher includes a large number of variable transformations in the regression. Example: since the true functional form is unknown, the researcher adds interaction terms, variables with functional transformation, etc. such that the number of regressors is greater than the number of observations. <br>\n",
    "**What if $P>N$?** <br>\n",
    "When the number of regressors is greater than the number of observations, the regression cannot be identified using OLS <br>\n",
    "**What if $P=N$?** <br>\n",
    "When the number of regressors is equal to the number of observations, the OLS fits the data perfectly such that $R^2=1$ <br>\n",
    "**What if $P<N$?** <br>\n",
    "When the number of regressors is smaller than the number of observations, we will not only be able to identify the model but also avoid overfitting issues as otherwise. When we have high-dimensional data, we need to reduce the dimensions (regularize) such that $P<N$ to draw meaningful conclusions from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Theoretical Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Approximately Sparse Regression Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors consider the following model to forecast the outcome ($y_i$) and treat $g(w_i)$ as a high-dimensional, linear model such that <br>\n",
    "<br>\n",
    "$$y_i=g(w_i)+\\zeta_i \\hspace{0.1cm}, \\hspace{0.25cm} i=1,...n \\hspace{0.25cm} \\& \\hspace{0.25cm} E(\\zeta_i|w_i)=0$$ <br>\n",
    "$$g(w_i) =\\sum_{j=1}^{p}{\\beta_jx_{i,j}}+r_{p,i} \\hspace{0.25cm}$$\n",
    "<div style=\"text-align: justify\">   \n",
    "$r_{p,i}$ is an approximation error. To identify the model, avoid overfitting and for better prediction ability, we need to regularize the function $g(\\cdot)$. Typically, researchers regularize based on economic theory and intuition. However, the question of the correct variables and the transformations being chosen is left unanswered. Another approach to reduce dimensions is the approximate sparsity of the high-dimensional linear model. <br>\n",
    "</div>\n",
    "<blockquote>“Approximate sparsity imposes a restriction that only $s$ variables among all of $x_{i,j}$, where $s$ is much smaller than $n$, have associated coefficients $\\beta_j$ that are different from $0$, while permitting a nonzero approximation error $r_{p,i}$.” </blockquote>\n",
    "This implies only S of the P regressors have coefficients that are not equal to zero and $S<P<N$. One of the methods for estimating the parameter of sparse high-dimensional models is the Least Absolute Shrinkage and Selection Operator (LASSO). In this paper, the authors use a variant of LASSO estimator defined as: <br> \n",
    "<br>    \n",
    "$$\\hat{\\beta}=\\mathop{\\rm argmax}\\limits_b\\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}x_{i,j}b_j)^2+\\lambda\\sum_{j=1}^{p}|b_j|\\gamma_j \\hspace{0.25cm}$$\n",
    "<br>  \n",
    "$\\lambda>0$ is the penalty and $\\gamma_j$ are the penalty loadings. LASSO leads to some of the coefficients being set exactly to zero. Thus, it could be used in variable selection process by simply selecting the variables with nonzero coefficients. This addresses the problem when researcher doesn’t know a priori exactly which variables should in the model. <br> \n",
    "<br>      \n",
    "    \n",
    "This variant is proposed in [Belloni, Chen, Chernozhukov, and Hansen (2012)](https://www.econometricsociety.org/publications/econometrica/2012/11/01/sparse-models-and-methods-optimal-instruments-application)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Selection when the goal is Causal Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aim of this paper is to use data mining procedures like LASSO for regularizing high-dimensional data to produce meaningful inferences. Authors stress that when the end goal is causal inference, LASSO should be used only for regularization purposes i.e, to only select variables whose coefficients are nonzero but not to interpret their coefficients directly. This is because: <br> \n",
    "1. Such procedures are designed for forecasting purposes and not to inference about model parameters <br> \n",
    "2. Model selection mistakes occur which then might lead to the problem of omitted variable bias <br>  \n",
    "\n",
    "Thus, naively using results obtained from LASSO estimation might lead to inference problems. The authors explain when and how to use LASSO when the end goal of the researcher is causal inference about model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inference with Selection among many Instruments (Z)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors consider the following linear instrumental variable model <br>\n",
    "$$y_i=\\alpha d_i+\\varepsilon_i \\hspace{0.25cm} where \\hspace{0.25cm} d_i={z_i}^{'}\\Pi+r_i+\\nu_i$$ <br>\n",
    "$$E(\\varepsilon_i|z_i)=E(\\nu_i|z_i,r_i)=0 \\hspace{0.25cm} but \\hspace{0.25cm} E(\\varepsilon_i|\\nu_i)\\ne0$$ <br>\n",
    "$d_i$ is an endogenous variable, $z_i$ is a p-dimensional vector of instruments and $r_i$ is an approximation error. In this scenario, the number of instruments is greater than the number of observations $(Z>N)$ which calls for selection among the instruments to be able to estimate $\\alpha$ <br>\n",
    "\n",
    "**Choosing instruments in case of one endogenous variable:** <br>\n",
    "Step 1: Use LASSO to regress $d_i$ (endogenous variable) on $z_i$ (instrumental variables) <br>\n",
    "Step 2: Select the instrument(s) with nonzero coefficients from step 1 <br>\n",
    "Step 3: Carry out standard 2SLS estimation using the selected instrument(s) from step 2 <br>\n",
    "\n",
    "**Choosing instruments in case of more than one endogenous variable:** <br>\n",
    "Step 1: Use LASSO to regress $d_i$ on $z_i$ for each of the endogenous variables <br>\n",
    "Step 2: Select the instrument(s) with nonzero coefficients from step 1 for each of the endogenous variables <br>\n",
    "Step 3: Carry out standard 2SLS estimation using the union of the selected instruments from step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inference with Selection among many Controls (C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors consider the following linear model where a treatment variable, $d_i$, is taken as exogenous after conditioning on control variables <br>\n",
    "\n",
    "\n",
    "$$\\tag{1}\\label{eq:1}y_i=\\alpha d_i+ x_i^{'}\\theta_y+r_{yi}+\\zeta_i \\hspace{0.1cm} ,\\hspace{0.25cm} E(\\zeta_i|d_i,x_i,r_{yi})=0$$\n",
    "<div style=\"text-align: justify\"> $x_i$ is a p-dimensional vector of controls and $r_{yi}$ is an approximation error. In this scenario, the number of controls is greater than the number of observations $(C>N)$ which calls for selection among the controls to be able to estimate $\\alpha$ which is the effect of the treatment on the outcome. To ensure $\\alpha$ remains in the model as it is the parameter of interest, one should exclude the treatment variable from the model selection process. Thus, regularization will only be carried out on the remaining exogenous variables (controls) which are referred to as ‘nuisance’ by the authors. </div> <br>\n",
    "\n",
    "**Naïve Approach for Selection among many Controls (C)**  <br>\n",
    "Step 1: Use LASSO to regress $y_i$ (outcome variable) on $x_i$ (control variables) <br>\n",
    "Step 2: Select the control(s) with nonzero coefficients from step 1 <br>\n",
    "Step 3: Carry out standard OLS estimation using the selected control(s) from step 2\n",
    "<div style=\"text-align: justify\">Running LASSO with $y_i$ on $x_i$ (step 1) only allows us to select those variables with nonzero coefficients associated to $y_i$. However, it ignores the relationship between treatment variable and controls leading to omitted variable bias. This is a major drawback of using the naïve approach. To understand the relationship between treatment variable and controls, the following equations are introduced by the authors </div> <br>\n",
    "\n",
    "$$\\tag{2}\\label{eq:2}d_i=x_i^{'}\\theta_d+r_{di}+\\nu_i \\hspace{0.1cm} ,\\hspace{0.25cm} E(\\nu_i|x_i,r_{di})=0$$ <br>\n",
    "\n",
    "$$\\tag{3}\\label{eq:3}y_i=x_i^{'}(\\alpha\\theta_d+\\theta_y)+(\\alpha r_{di}+r_{yi})+(\\alpha\\nu_i+\\zeta_i)=x_i^{'}\\pi+r_{ci}+\\varepsilon_i \\hspace{0.1cm} ,\\hspace{0.25cm} E(\\varepsilon_i|x_i,r_{ci})=0$$ <br>\n",
    "\n",
    "<div style=\"text-align: justify\">Substituting equation \\eqref{eq:2} in \\eqref{eq:1} will yield \\eqref{eq:3}. $r_{ci}$ is a composite approximation error. Spare high-dimensional methods work best for predictions and not inferences. Equation \\eqref{eq:1} represents a structural model where the goal to is to learn the causal inference. Whereas, equations \\eqref{eq:2} and \\eqref{eq:3} represent predictive relationships, which can be estimated by high-dimensional methods. Thus, to counter against the omitted variable bias, we use both the equations to select the controls. This is called the Post-Double Selection (PDS) LASSO.</div> <br>\n",
    "\n",
    "**Post-Double-Selection (PDS) for Selection among many Controls (C)** <br>\n",
    "Step 1: Use LASSO to regress $y_i$ (outcome variable) on $x_i$ (control variables) <br>\n",
    "Step 2: Select the control(s) with nonzero coefficients from step 1 <br>\n",
    "Step 3: Use LASSO to regress $d_i$ (treatment variable) on $x_i$ (control variables) <br>\n",
    "Step 4: Select the control(s) with nonzero coefficients from step 3 <br>\n",
    "Step 5: Carry out standard OLS estimation using the union of selected controls from step 2 and step 4 <br>\n",
    "\n",
    "<img src=\"simulation copy.png\" width =\"500\" height=500>\n",
    "\n",
    "\n",
    "Figure 1 is a simulation study from [Belloni, Chernozhukov, and Hansen (2014)](https://academic.oup.com/restud/article-abstract/81/2/608/1523757?redirectedFrom=fulltext). It compares the sampling distribution of the estimator of $\\alpha$ from naïve approach and PDS approach. The left panel suffers from the omitted variable bias which is absent in the right panel.<br>\n",
    "The authors use three empirical examples in this paper and implement the LASSO for selecting controls and instruments in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Empirical Example 1: Estimating the Impact of Eminent Domain on House Prices**\n",
    "\n",
    "*Chen, Daniel L., and Susan Yeh. 2012. [“Growth under the Shadow of Expropriation? The Economic Impacts of Eminent Domain.”](http://www.sole-jole.org/13463.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Eminent Domain also known as land acquisition or expropriation is the power of the government to take away private property, in exchange for some money, for public use. The US constitution gives this right to the government under the Fifth Amendment called the Takings Clause. Pro-plaintiff rulings are federal court rulings that say that the government seizure of a private property was unlawful. In the paper by Chen and Yeh (2012), they try to find the effect of such federal court decisions regarding eminent domain on housing prices. <br>\n",
    "<br>\n",
    "$$log(Case–Shiller_{ct})=\\alpha\\cdot TakingsLaw_{ct}+\\beta_c+\\beta_t+\\gamma_{c}t+W_{ct}^{'}\\delta+\\varepsilon_{ct}$$\n",
    "<br>\n",
    "$Case–Shiller_{ct}$ is the average of the Case–Shiller home price index within circuit court $c$ at time $t$. $TakingsLaw_{ct}$ represents the number of pro-plaintiff appellate takings decisions in federal circuit court $c$ and year $t$. $W_{ct}$ are exogenous variables and $\\beta_c$, $\\beta_t$ and $\\gamma_{c}t$ are respectively circuit-specific effects, time-specific effects, and circuit-specific time trends. The parameter of interest, $\\alpha$, thus represents the effect of an additional pro-plaintiff decision on property prices.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2. Identification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The model presented above suffers from endogeneity issue. In their paper, Chen and Yeh (2012), state that court decisions are endogenous. For instance, the pro-plaintiff ruling may be more likely if housing prices in an area are expected to increase. To counter this problem, they employ an instrumental variable approach and consider the characteristics of judges to be the instruments for judicial decisions. Judges are randomly assigned to three-judge panels. Since the characteristics of judges are unrelated to any other factors except for their judicial decisions that may affect the housing prices, the instruments are argued to be valid. <br>\n",
    "</div>    \n",
    "<img src=\"CG1.png\" width =\"300\" height=300>\n",
    "<div style=\"text-align: justify\">\n",
    "The above causal graph illustrates the relationship between pro-plaintiff decisions (D) and housing prices (Y). Since judicial decisions (D) are endogenous, the estimates of regressing Y on D will be biased. ?????Explain the backdoor path???? To resolve this, the authors use the characteristics of judges (Z) as an instrument and argue it to be valid. Thus Z satisfies the exclusion restriction and effects Y only through D. <br>\n",
    "The characteristics of judges include their preferences over governments and individual property rights, gender, race, religion, political party affiliation, source of academic degrees, and if the judges had been promoted from a district court. The list of potential instruments not only included these characteristics for each judge but also interaction terms with various combinations. The data set consists of 183 observations, 72 control variables, and 147 instrumental variables. The model cannot be identified as $N(183)<P(219)$. To estimate this model for causal inference, there is a need to regularize. <br>\n",
    "In this paper, Belloni et al. (2014), authors use a LASSO variant for reducing the dimensions by selecting among many instruments. They wrote a program for the same due to which, using glmnet package will not give the same results as theirs. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3. Replication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 149)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data\n",
    "df1=pd.read_excel(\"/Users/satwika/Desktop/PROJECT/Data/JEPReplicationFilesStata/EminentDomain/CSExampleData.xlsx\")\n",
    "df=df1.drop('nControl',axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSIndex</th>\n",
       "      <th>NumProCase</th>\n",
       "      <th>Z1xNR</th>\n",
       "      <th>Z1xJDP</th>\n",
       "      <th>Z1xD</th>\n",
       "      <th>Z2xD</th>\n",
       "      <th>Z3xD</th>\n",
       "      <th>Z1xDSq</th>\n",
       "      <th>Z1xDCu</th>\n",
       "      <th>Z1xF</th>\n",
       "      <th>...</th>\n",
       "      <th>Z1xPROC1xM</th>\n",
       "      <th>Z1xPROC1xEV</th>\n",
       "      <th>Z1xPROC1xPRO</th>\n",
       "      <th>Z1xPROC1xNW</th>\n",
       "      <th>Z1xMC1xJ</th>\n",
       "      <th>Z1xMC1xCAT</th>\n",
       "      <th>Z1xMC1xNR</th>\n",
       "      <th>Z1xMC1xEV</th>\n",
       "      <th>Z1xMC1xM</th>\n",
       "      <th>Z1xMC1xNW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071146</td>\n",
       "      <td>0.119098</td>\n",
       "      <td>-0.067744</td>\n",
       "      <td>0.102502</td>\n",
       "      <td>-0.024818</td>\n",
       "      <td>0.247578</td>\n",
       "      <td>0.085026</td>\n",
       "      <td>-0.375188</td>\n",
       "      <td>-1.487713</td>\n",
       "      <td>0.091808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.480826</td>\n",
       "      <td>0.096378</td>\n",
       "      <td>0.210369</td>\n",
       "      <td>-0.267153</td>\n",
       "      <td>-0.127030</td>\n",
       "      <td>-0.055966</td>\n",
       "      <td>-0.341225</td>\n",
       "      <td>-0.139522</td>\n",
       "      <td>-0.460450</td>\n",
       "      <td>-0.658211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024656</td>\n",
       "      <td>-0.082198</td>\n",
       "      <td>0.102703</td>\n",
       "      <td>0.198287</td>\n",
       "      <td>0.010865</td>\n",
       "      <td>0.179050</td>\n",
       "      <td>-0.021043</td>\n",
       "      <td>0.510609</td>\n",
       "      <td>3.374342</td>\n",
       "      <td>0.076370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>-0.044288</td>\n",
       "      <td>0.593042</td>\n",
       "      <td>0.887688</td>\n",
       "      <td>0.306955</td>\n",
       "      <td>-0.069646</td>\n",
       "      <td>0.282035</td>\n",
       "      <td>-0.092997</td>\n",
       "      <td>-0.004318</td>\n",
       "      <td>0.536386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.051935</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.126258</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.540937</td>\n",
       "      <td>-0.282949</td>\n",
       "      <td>0.102560</td>\n",
       "      <td>-0.123052</td>\n",
       "      <td>2.311071</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>...</td>\n",
       "      <td>1.352279</td>\n",
       "      <td>0.231767</td>\n",
       "      <td>0.699562</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>0.808672</td>\n",
       "      <td>0.291625</td>\n",
       "      <td>-0.050386</td>\n",
       "      <td>0.585404</td>\n",
       "      <td>1.199408</td>\n",
       "      <td>0.485358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.017423</td>\n",
       "      <td>-0.037161</td>\n",
       "      <td>-0.052893</td>\n",
       "      <td>-0.214215</td>\n",
       "      <td>0.164018</td>\n",
       "      <td>-0.413705</td>\n",
       "      <td>-0.100744</td>\n",
       "      <td>-0.272045</td>\n",
       "      <td>-3.395065</td>\n",
       "      <td>0.073229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496520</td>\n",
       "      <td>0.083141</td>\n",
       "      <td>-1.442539</td>\n",
       "      <td>-0.490165</td>\n",
       "      <td>-0.981868</td>\n",
       "      <td>0.242997</td>\n",
       "      <td>0.054547</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>-0.141818</td>\n",
       "      <td>0.078161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.057601</td>\n",
       "      <td>0.096159</td>\n",
       "      <td>0.144511</td>\n",
       "      <td>0.150797</td>\n",
       "      <td>0.495104</td>\n",
       "      <td>0.244553</td>\n",
       "      <td>-0.020875</td>\n",
       "      <td>0.619919</td>\n",
       "      <td>-0.361951</td>\n",
       "      <td>0.123615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481784</td>\n",
       "      <td>-0.712932</td>\n",
       "      <td>-0.682744</td>\n",
       "      <td>-1.148617</td>\n",
       "      <td>0.231269</td>\n",
       "      <td>-0.062621</td>\n",
       "      <td>0.361904</td>\n",
       "      <td>0.094149</td>\n",
       "      <td>0.447288</td>\n",
       "      <td>-0.603682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CSIndex  NumProCase     Z1xNR    Z1xJDP      Z1xD      Z2xD      Z3xD  \\\n",
       "0  0.071146    0.119098 -0.067744  0.102502 -0.024818  0.247578  0.085026   \n",
       "1  0.024656   -0.082198  0.102703  0.198287  0.010865  0.179050 -0.021043   \n",
       "2  0.051935   -0.001592 -0.126258 -0.000265 -0.540937 -0.282949  0.102560   \n",
       "3 -0.017423   -0.037161 -0.052893 -0.214215  0.164018 -0.413705 -0.100744   \n",
       "4 -0.057601    0.096159  0.144511  0.150797  0.495104  0.244553 -0.020875   \n",
       "\n",
       "     Z1xDSq    Z1xDCu      Z1xF  ...  Z1xPROC1xM  Z1xPROC1xEV  Z1xPROC1xPRO  \\\n",
       "0 -0.375188 -1.487713  0.091808  ...   -0.480826     0.096378      0.210369   \n",
       "1  0.510609  3.374342  0.076370  ...    0.129356    -0.044288      0.593042   \n",
       "2 -0.123052  2.311071  0.005722  ...    1.352279     0.231767      0.699562   \n",
       "3 -0.272045 -3.395065  0.073229  ...   -0.496520     0.083141     -1.442539   \n",
       "4  0.619919 -0.361951  0.123615  ...    0.481784    -0.712932     -0.682744   \n",
       "\n",
       "   Z1xPROC1xNW  Z1xMC1xJ  Z1xMC1xCAT  Z1xMC1xNR  Z1xMC1xEV  Z1xMC1xM  \\\n",
       "0    -0.267153 -0.127030   -0.055966  -0.341225  -0.139522 -0.460450   \n",
       "1     0.887688  0.306955   -0.069646   0.282035  -0.092997 -0.004318   \n",
       "2    -0.208514  0.808672    0.291625  -0.050386   0.585404  1.199408   \n",
       "3    -0.490165 -0.981868    0.242997   0.054547  -0.006271 -0.141818   \n",
       "4    -1.148617  0.231269   -0.062621   0.361904   0.094149  0.447288   \n",
       "\n",
       "   Z1xMC1xNW  \n",
       "0  -0.658211  \n",
       "1   0.536386  \n",
       "2   0.485358  \n",
       "3   0.078161  \n",
       "4  -0.603682  \n",
       "\n",
       "[5 rows x 149 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:             NumProCase   R-squared (uncentered):                   0.183\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.178\n",
      "Method:                 Least Squares   F-statistic:                              40.66\n",
      "Date:                Tue, 16 Jun 2020   Prob (F-statistic):                    1.45e-09\n",
      "Time:                        15:48:36   Log-Likelihood:                         -7.0982\n",
      "No. Observations:                 183   AIC:                                      16.20\n",
      "Df Residuals:                     182   BIC:                                      19.41\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Z1xJDPSq       0.4495      0.070      6.377      0.000       0.310       0.589\n",
      "==============================================================================\n",
      "Omnibus:                       29.079   Durbin-Watson:                   2.497\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               46.677\n",
      "Skew:                           0.846   Prob(JB):                     7.31e-11\n",
      "Kurtosis:                       4.805   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffbcdc3d1d0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First Stage Least Square with instrument selected by the authors using lassoshooting method\n",
    "rslt_fs= smf.ols(formula=\"NumProCase ~ Z1xJDPSq - 1\", data=df).fit(use_t=\"none\")\n",
    "print(rslt_fs.summary())\n",
    "plt.plot([\"Z1xJDPSq\"], [\"NumProCase\"]) #Relevance: the result of the first stage and the graph show how relevant Z=>X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                CSIndex   R-squared (uncentered):                   0.024\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.019\n",
      "Method:                 Least Squares   F-statistic:                              6.981\n",
      "Date:                Tue, 16 Jun 2020   Prob (F-statistic):                     0.00896\n",
      "Time:                        15:48:38   Log-Likelihood:                          286.84\n",
      "No. Observations:                 183   AIC:                                     -571.7\n",
      "Df Residuals:                     182   BIC:                                     -568.5\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:                  HC1                                                  \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "predicted_NumProCase     0.0668      0.025      2.642      0.009       0.017       0.117\n",
      "==============================================================================\n",
      "Omnibus:                        2.118   Durbin-Watson:                   0.791\n",
      "Prob(Omnibus):                  0.347   Jarque-Bera (JB):                1.773\n",
      "Skew:                          -0.229   Prob(JB):                        0.412\n",
      "Kurtosis:                       3.151   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n"
     ]
    }
   ],
   "source": [
    "#Second Stage Least Square with instrument selected by the authors using lassoshooting method\n",
    "df['predicted_NumProCase'] = rslt_fs.predict()\n",
    "rslt_ss= smf.ols(formula=\"CSIndex ~ predicted_NumProCase - 1\", data=df).fit(use_t=\"none\")\n",
    "rslt_ss_robust = rslt_ss.get_robustcov_results()\n",
    "print(rslt_ss_robust.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:             NumProCase   R-squared (uncentered):                   0.005\n",
      "Model:                            OLS   Adj. R-squared (uncentered):             -0.001\n",
      "Method:                 Least Squares   F-statistic:                             0.8666\n",
      "Date:                Tue, 16 Jun 2020   Prob (F-statistic):                       0.353\n",
      "Time:                        15:48:40   Log-Likelihood:                         -25.114\n",
      "No. Observations:                 183   AIC:                                      52.23\n",
      "Df Residuals:                     182   BIC:                                      55.44\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Z1xD           0.0664      0.071      0.931      0.353      -0.074       0.207\n",
      "==============================================================================\n",
      "Omnibus:                       22.241   Durbin-Watson:                   2.582\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               32.759\n",
      "Skew:                           0.699   Prob(JB):                     7.70e-08\n",
      "Kurtosis:                       4.531   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffbce7ef0d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First Stage Least Square with instrument selected by the authors for comparision\n",
    "cmpr_fs= smf.ols(formula=\"NumProCase ~ Z1xD - 1\", data=df).fit(use_t=\"none\")\n",
    "print(cmpr_fs.summary())\n",
    "plt.plot([\"Z1xD\"], [\"NumProCase\"]) #Relevance: the result of the first stage and the graph show how relevant Z=>X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                CSIndex   R-squared (uncentered):                   0.008\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.002\n",
      "Method:                 Least Squares   F-statistic:                              1.804\n",
      "Date:                Tue, 16 Jun 2020   Prob (F-statistic):                       0.181\n",
      "Time:                        15:48:43   Log-Likelihood:                          285.32\n",
      "No. Observations:                 183   AIC:                                     -568.6\n",
      "Df Residuals:                     182   BIC:                                     -565.4\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:                  HC1                                                  \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "predicted_NumProCase_cmpr    -0.2368      0.176     -1.343      0.181      -0.585       0.111\n",
      "==============================================================================\n",
      "Omnibus:                        2.462   Durbin-Watson:                   0.802\n",
      "Prob(Omnibus):                  0.292   Jarque-Bera (JB):                2.201\n",
      "Skew:                          -0.266   Prob(JB):                        0.333\n",
      "Kurtosis:                       3.074   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n"
     ]
    }
   ],
   "source": [
    "#Second Stage Least Square with instrument selected by the authors for comparision\n",
    "df['predicted_NumProCase_cmpr'] = cmpr_fs.predict()\n",
    "cmpr_ss= smf.ols(formula=\"CSIndex ~ predicted_NumProCase_cmpr - 1\", data=df).fit(use_t=\"none\")\n",
    "cmpr_ss_robust = cmpr_ss.get_robustcov_results()\n",
    "print(cmpr_ss_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Contribution for Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Stage Least Square with all the instruments\n",
    "Y = df.iloc[:,1]\n",
    "#Y\n",
    "X = df.iloc[:,2:]\n",
    "#X\n",
    "all_fs= smf.ols(formula=\"Y ~ X - 1\", data=df).fit(use_t=\"none\")\n",
    "print(all_fs.summary())\n",
    "#Plot correlation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Stage Least Square with all the instruments\n",
    "df['predicted_NumProCase_all'] = all_fs.predict()\n",
    "all_ss= smf.ols(formula=\"CSIndex ~ predicted_NumProCase_all - 1\", data=df).fit(use_t=\"none\")\n",
    "all_ss_robust = all_ss.get_robustcov_results()\n",
    "print(all_ss_robust.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Stage Least Square using the instruments selected by glmnet on R\n",
    "glmnet_fs= smf.ols(formula=\"NumProCase ~ Z1xPRO + Z2xJDPC1xJ - 1\", data=df).fit(use_t=\"none\")\n",
    "print(glmnet_fs.summary())\n",
    "#Plot correlation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Stage Least Square using the instruments selected by glmnet on R\n",
    "df['predicted_NumProCase_glmnet'] = glmnet_fs.predict()\n",
    "glmnet_ss= smf.ols(formula=\"CSIndex ~ predicted_NumProCase_glmnet - 1\", data=df).fit(use_t=\"none\")\n",
    "glmnet_ss_robust = glmnet_ss.get_robustcov_results()\n",
    "print(glmnet_ss_robust.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table comparing all the results:\n",
    "#1. All instruments \n",
    "#2. Instruments which authors chose\n",
    "#3. Iinstruments from glmnet \n",
    "#4. Instrument based on intruition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robustness checks: Refer to other paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
